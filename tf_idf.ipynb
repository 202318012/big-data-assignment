{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Manthan Solanki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID:   202318012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1705571290128,
     "user": {
      "displayName": "2023 18012",
      "userId": "12537447585000372464"
     },
     "user_tz": -330
    },
    "id": "Om5RwG4Rx4ho"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You need to implement a TF-IDF vectorizer to convert a collection of documents \n",
    "into TF-IDF vectors. You can use the sklearnâ€™s inbuild datase        \r\n",
    "fetch_20newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD-IDF vectors:  \n",
      "  (0, 86416)\t0.14330464297977982\n",
      "  (0, 35135)\t0.10188109676312235\n",
      "  (0, 65968)\t0.10658183340971177\n",
      "  (0, 114195)\t0.06002582888934523\n",
      "  (0, 78809)\t0.06524029473980168\n",
      "  (0, 76578)\t0.0752490171119318\n",
      "  (0, 57203)\t0.16977226500364592\n",
      "  (0, 67023)\t0.07965653370342658\n",
      "  (0, 63238)\t0.09086750717799585\n",
      "  (0, 95944)\t0.11792442679286105\n",
      "  (0, 127721)\t0.0660283455431985\n",
      "  (0, 109044)\t0.11811852219269026\n",
      "  (0, 51651)\t0.10581100308545811\n",
      "  (0, 83103)\t0.09633120317294654\n",
      "  (0, 113755)\t0.1926949257821117\n",
      "  (0, 73061)\t0.04662587301170703\n",
      "  (0, 34131)\t0.09493746671845804\n",
      "  (0, 101175)\t0.08899924936054199\n",
      "  (0, 105907)\t0.10749912859686628\n",
      "  (0, 35560)\t0.1446512460011004\n",
      "  (0, 26070)\t0.10385185139503332\n",
      "  (0, 108033)\t0.08197182211166716\n",
      "  (0, 99619)\t0.06171903092868097\n",
      "  (0, 48552)\t0.1263844988551673\n",
      "  (0, 34943)\t0.18203649549572573\n",
      "  :\t:\n",
      "  (11313, 106061)\t0.11739285034416508\n",
      "  (11313, 67469)\t0.07888902121089117\n",
      "  (11313, 63763)\t0.11511167728184264\n",
      "  (11313, 37359)\t0.16552340600580592\n",
      "  (11313, 116769)\t0.08740758023936664\n",
      "  (11313, 72166)\t0.10891263256743795\n",
      "  (11313, 11390)\t0.14477754748280827\n",
      "  (11313, 60694)\t0.08134974610767784\n",
      "  (11313, 113581)\t0.0749042118526549\n",
      "  (11313, 62582)\t0.0632144659864555\n",
      "  (11313, 3411)\t0.07079755573089706\n",
      "  (11313, 76233)\t0.06469398052315968\n",
      "  (11313, 119441)\t0.060284105803626004\n",
      "  (11313, 47916)\t0.049639180787655675\n",
      "  (11313, 88185)\t0.14159511146179413\n",
      "  (11313, 111466)\t0.08179695308516817\n",
      "  (11313, 51651)\t0.10242810228694169\n",
      "  (11313, 4605)\t0.06676826497088761\n",
      "  (11313, 75888)\t0.020264179021749786\n",
      "  (11313, 90192)\t0.021012136747188718\n",
      "  (11313, 63970)\t0.037346306116846265\n",
      "  (11313, 94962)\t0.03634515160466896\n",
      "  (11313, 87451)\t0.037610885317695526\n",
      "  (11313, 111094)\t0.020198023351223362\n",
      "  (11313, 50455)\t0.057582965641085816\n"
     ]
    }
   ],
   "source": [
    "# for vectorizing documents\n",
    "def get_vecs(data):\n",
    "    \n",
    "    vec = TfidfVectorizer(stop_words = 'english')\n",
    "    mat = vec.fit_transform(data)\n",
    "    return mat\n",
    "    \n",
    "data = fetch_20newsgroups(subset='train')\n",
    "# data = data['data']\n",
    "\n",
    "tfidf_vecs = get_vecs(data['data'])\n",
    "print('TD-IDF vectors:  ')\n",
    "print(tfidf_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a function to calculate the cosine similarity between two TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:  From: chen@citr.uq.o\n",
      "\n",
      "Document 2:  From: ljbartel@naomi\n",
      "\n",
      "Cosine Similarity between docs:  0.002769\n"
     ]
    }
   ],
   "source": [
    "# to get cosine similarity\n",
    "\n",
    "def get_csn_sim(v1, v2):\n",
    "    v1 = v1.reshape(len(v1), -1)\n",
    "    v2 = v2.reshape(len(v2), -1)\n",
    "    csn_sim = cosine_similarity(v1, v2)\n",
    "    return csn_sim\n",
    "    \n",
    "\n",
    "tfidf_vecs = get_vecs(data['data'])\n",
    "\n",
    "ind1 = np.random.randint(0, tfidf_vecs.shape[0])\n",
    "ind2 = np.random.randint(0, tfidf_vecs.shape[0])\n",
    "\n",
    "print('\\nDocument 1: ', data['data'][ind1][:20])\n",
    "print('\\nDocument 2: ', data['data'][ind2][:20])\n",
    "\n",
    "# cosine similarity between two docs\n",
    "\n",
    "csn_sim = cosine_similarity(tfidf_vecs[ind1], tfidf_vecs[ind2])\n",
    "csn_sim = np.round(csn_sim[0][0], 6) \n",
    "print('\\nCosine Similarity between docs: ', csn_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine Similarity between docs: \n",
      "\n",
      " [[0.002264 0.014679 0.037178 ... 0.00314  0.004641 0.005051]\n",
      " [0.012259 0.004546 0.00854  ... 0.005402 0.004967 0.005188]\n",
      " [0.009683 0.003884 0.006585 ... 0.006869 0.003635 0.00605 ]\n",
      " ...\n",
      " [0.055665 0.034321 0.007467 ... 0.00597  0.004286 0.014023]\n",
      " [0.015562 0.017432 0.016889 ... 0.019029 0.006591 0.02091 ]\n",
      " [0.010691 0.01063  0.00663  ... 0.008187 0.006292 0.008323]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx = np.array([x for x in range(tfidf_vecs.shape[0])])\n",
    "\n",
    "# splitting documents into train and test to get similarity\n",
    "train_idx = np.random.choice(idx, size = int(0.8 * (tfidf_vecs.shape[0])), replace = False)\n",
    "test_idx = np.setdiff1d(idx, train_idx)\n",
    "\n",
    "train = tfidf_vecs[train_idx]\n",
    "test = tfidf_vecs[test_idx]\n",
    "\n",
    "csn_sim = cosine_similarity(train, test)\n",
    "csn_sim = np.round(csn_sim, 6) \n",
    "print('\\nCosine Similarity between docs: \\n\\n', csn_sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement a document similarity search function that takes a document as input\n",
    "and returns a list of documents ranked by their similarity to the input document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document similarity search function\n",
    "\n",
    "def doc_sim(idx):\n",
    "\n",
    "    data = fetch_20newsgroups(subset='train')\n",
    "    docs = data['data']\n",
    "    d = docs[idx]\n",
    "    \n",
    "    docs.pop(idx)\n",
    "    vec = TfidfVectorizer(stop_words = 'english')\n",
    "    train = vec.fit_transform(docs)\n",
    "    test = vec.transform([d])\n",
    "\n",
    "    csn_sim = cosine_similarity(train, test)\n",
    "    print('sim: ', csn_sim)\n",
    "    indices = np.arange(len(csn_sim)).astype('int')\n",
    "    print(\"indices:  \", indices)\n",
    "    \n",
    "    # csn_sim = np.append(csn_sim, indices, axis=1)\n",
    "    \n",
    "    sim_idx = np.argsort(csn_sim, axis=0).flatten()\n",
    "    print('\\nsim_idx: ', sim_idx)\n",
    "    top_5 = sim_idx.argsort()[:-6:-1]\n",
    "    print('min indices: ',   top_5)\n",
    "\n",
    "    \n",
    "    \n",
    "    res = [docs[i][:30] for i in top_5]\n",
    "    \n",
    "    return np.array (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim:  [[0.01702913]\n",
      " [0.00676701]\n",
      " [0.01859057]\n",
      " ...\n",
      " [0.003967  ]\n",
      " [0.01170542]\n",
      " [0.0147781 ]]\n",
      "indices:   [    0     1     2 ... 11310 11311 11312]\n",
      "\n",
      "sim_idx:  [4772 8665 9080 ... 1042 6431  890]\n",
      "min indices:  [8864 7804 2991 2583 2632]\n",
      "List of Similar Documents: \n",
      "\n",
      " ['From: stusoft@hardy.u.washingt' 'From: klee@synoptics.com (Ken '\n",
      " 'From: \"danny hawrysio\" <danny.' 'From: maynard@ramsey.cs.lauren'\n",
      " 'From: vbv@r2d2.eeap.cwru.edu (']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx = np.random.randint(0, len(data['data']))\n",
    "idx = doc_sim(idx)\n",
    "\n",
    "print(\"List of Similar Documents: \\n\\n\", idx)\n",
    "\n",
    "# for i in idx:\n",
    "#     print(data['data'][i][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPr/jl0BwipGfOdh04SJjyQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
