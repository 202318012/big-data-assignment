{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Manthan Solanki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID:   202318012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1705571290128,
     "user": {
      "displayName": "2023 18012",
      "userId": "12537447585000372464"
     },
     "user_tz": -330
    },
    "id": "Om5RwG4Rx4ho"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You need to implement a TF-IDF vectorizer to convert a collection of documents \n",
    "into TF-IDF vectors. You can use the sklearnâ€™s inbuild datase        \r\n",
    "fetch_20newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD-IDF vectors:  \n",
      "  (0, 86416)\t0.14330464297977982\n",
      "  (0, 35135)\t0.10188109676312235\n",
      "  (0, 65968)\t0.10658183340971177\n",
      "  (0, 114195)\t0.06002582888934523\n",
      "  (0, 78809)\t0.06524029473980168\n",
      "  (0, 76578)\t0.0752490171119318\n",
      "  (0, 57203)\t0.16977226500364592\n",
      "  (0, 67023)\t0.07965653370342658\n",
      "  (0, 63238)\t0.09086750717799585\n",
      "  (0, 95944)\t0.11792442679286105\n",
      "  (0, 127721)\t0.0660283455431985\n",
      "  (0, 109044)\t0.11811852219269026\n",
      "  (0, 51651)\t0.10581100308545811\n",
      "  (0, 83103)\t0.09633120317294654\n",
      "  (0, 113755)\t0.1926949257821117\n",
      "  (0, 73061)\t0.04662587301170703\n",
      "  (0, 34131)\t0.09493746671845804\n",
      "  (0, 101175)\t0.08899924936054199\n",
      "  (0, 105907)\t0.10749912859686628\n",
      "  (0, 35560)\t0.1446512460011004\n",
      "  (0, 26070)\t0.10385185139503332\n",
      "  (0, 108033)\t0.08197182211166716\n",
      "  (0, 99619)\t0.06171903092868097\n",
      "  (0, 48552)\t0.1263844988551673\n",
      "  (0, 34943)\t0.18203649549572573\n",
      "  :\t:\n",
      "  (11313, 106061)\t0.11739285034416508\n",
      "  (11313, 67469)\t0.07888902121089117\n",
      "  (11313, 63763)\t0.11511167728184264\n",
      "  (11313, 37359)\t0.16552340600580592\n",
      "  (11313, 116769)\t0.08740758023936664\n",
      "  (11313, 72166)\t0.10891263256743795\n",
      "  (11313, 11390)\t0.14477754748280827\n",
      "  (11313, 60694)\t0.08134974610767784\n",
      "  (11313, 113581)\t0.0749042118526549\n",
      "  (11313, 62582)\t0.0632144659864555\n",
      "  (11313, 3411)\t0.07079755573089706\n",
      "  (11313, 76233)\t0.06469398052315968\n",
      "  (11313, 119441)\t0.060284105803626004\n",
      "  (11313, 47916)\t0.049639180787655675\n",
      "  (11313, 88185)\t0.14159511146179413\n",
      "  (11313, 111466)\t0.08179695308516817\n",
      "  (11313, 51651)\t0.10242810228694169\n",
      "  (11313, 4605)\t0.06676826497088761\n",
      "  (11313, 75888)\t0.020264179021749786\n",
      "  (11313, 90192)\t0.021012136747188718\n",
      "  (11313, 63970)\t0.037346306116846265\n",
      "  (11313, 94962)\t0.03634515160466896\n",
      "  (11313, 87451)\t0.037610885317695526\n",
      "  (11313, 111094)\t0.020198023351223362\n",
      "  (11313, 50455)\t0.057582965641085816\n"
     ]
    }
   ],
   "source": [
    "# for vectorizing documents\n",
    "def get_vecs(data):\n",
    "    \n",
    "    # vectorizing data \n",
    "    \n",
    "    vec = TfidfVectorizer(stop_words = 'english')\n",
    "    mat = vec.fit_transform(data)\n",
    "    return mat\n",
    "\n",
    "# fetching data from built-in sklearn dataset\n",
    "    \n",
    "data = fetch_20newsgroups(subset='train')\n",
    "# data = data['data']\n",
    "\n",
    "tfidf_vecs = get_vecs(data['data'])\n",
    "print('TD-IDF vectors:  ')\n",
    "print(tfidf_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a function to calculate the cosine similarity between two TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1:  From: aa229@Freenet.carleton.ca (Steve Birnbaum)\n",
      "Subject: Re: rejoinder. Questions to Israelis\n",
      "Reply\n",
      "\n",
      "Document 2:  From: rrn@po.CWRU.Edu (Robert R. Novitskey)\n",
      "Subject: The \"P24T\"\n",
      "Organization: Case Western Reserve U\n",
      "\n",
      "Cosine Similarity between docs:  0.007025\n"
     ]
    }
   ],
   "source": [
    "# to get cosine similarity\n",
    "\n",
    "def get_csn_sim(v1, v2):\n",
    "    csn_sim = cosine_similarity(v1, v2)\n",
    "    return csn_sim\n",
    "    \n",
    "\n",
    "tfidf_vecs = get_vecs(data['data'])\n",
    "\n",
    "# comparing two random documents from dataset\n",
    "\n",
    "ind1 = np.random.randint(0, tfidf_vecs.shape[0])\n",
    "ind2 = np.random.randint(0, tfidf_vecs.shape[0])\n",
    "\n",
    "print('\\nDocument 1: ', data['data'][ind1][:100])\n",
    "print('\\nDocument 2: ', data['data'][ind2][:100])\n",
    "\n",
    "# cosine similarity between two docs\n",
    "\n",
    "csn_sim = get_csn_sim(tfidf_vecs[ind1], tfidf_vecs[ind2])\n",
    "csn_sim = np.round(csn_sim[0][0], 6) \n",
    "print('\\nCosine Similarity between docs: ', csn_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine Similarity between all docs: \n",
      "\n",
      " [[0.005075 0.003693 0.004547 ... 0.003033 0.009611 0.006038]\n",
      " [0.008745 0.014985 0.002162 ... 0.002752 0.007665 0.000853]\n",
      " [0.046965 0.008516 0.002519 ... 0.041996 0.000728 0.009717]\n",
      " ...\n",
      " [0.022131 0.010423 0.008925 ... 0.004424 0.005731 0.013659]\n",
      " [0.014192 0.043459 0.025256 ... 0.006072 0.006742 0.020782]\n",
      " [0.012188 0.00103  0.034731 ... 0.018326 0.014238 0.000485]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx = np.array([x for x in range(tfidf_vecs.shape[0])])\n",
    "\n",
    "# splitting documents into train and test to get similarity\n",
    "train_idx = np.random.choice(idx, size = int(0.8 * (tfidf_vecs.shape[0])), replace = False)\n",
    "test_idx = np.setdiff1d(idx, train_idx)\n",
    "\n",
    "train = tfidf_vecs[train_idx]\n",
    "test = tfidf_vecs[test_idx]\n",
    "\n",
    "csn_sim = cosine_similarity(train, test)\n",
    "csn_sim = np.round(csn_sim, 6) \n",
    "print('\\nCosine Similarity between all docs: \\n\\n', csn_sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement a document similarity search function that takes a document as input\n",
    "and returns a list of documents ranked by their similarity to the input document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document similarity search function\n",
    "\n",
    "def doc_sim(idx):\n",
    "\n",
    "    data = fetch_20newsgroups(subset='train')\n",
    "    docs = data['data']\n",
    "    d = docs[idx]\n",
    "    \n",
    "    docs.pop(idx)\n",
    "    vec = TfidfVectorizer(stop_words = 'english')\n",
    "    train = vec.fit_transform(docs)\n",
    "    test = vec.transform([d])\n",
    "\n",
    "    csn_sim = cosine_similarity(train, test)\n",
    "    print('\\nCosine similarity matrix: \\n\\n', csn_sim)\n",
    "    indices = np.arange(len(csn_sim)).astype('int')\n",
    "    \n",
    "    sim_idx = np.argsort(csn_sim, axis=0).flatten()\n",
    "    print('\\nDocument indices sorted by index rank: \\n', sim_idx)\n",
    "    top_5 = sim_idx.argsort()[:-6:-1]\n",
    "    print('\\nMin. document indices: ',   top_5)\n",
    "    \n",
    "    res = [docs[i][:100] for i in top_5]\n",
    "    \n",
    "    return np.array (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Document to search: \n",
      "\n",
      ":  From: lady@uhunix.uhcc.Hawaii.Edu (Lee Lady)\n",
      "Subject: Re: Science and Methodology\n",
      "Summary: Merely av\n",
      "\n",
      "Cosine similarity matrix: \n",
      "\n",
      " [[0.01167803]\n",
      " [0.01063482]\n",
      " [0.0219113 ]\n",
      " ...\n",
      " [0.00201654]\n",
      " [0.02942423]\n",
      " [0.00673224]]\n",
      "\n",
      "Document indices sorted by index rank: \n",
      " [4772 8665 9080 ... 3705 5912 4471]\n",
      "Min. document indices:  [ 2523 10326   424  2372  1358]\n",
      "\n",
      "List of Most Similar Documents by ranking: \n",
      "\n",
      "\n",
      "1. From: envbvs@epb11.lbl.gov (Brian V. Smith)\n",
      "Subject: Re: I need source for splines\n",
      "Article-I.D.: dog\n",
      "\n",
      "2. From: amanda@intercon.com (Amanda Walker)\n",
      "Subject: Re: Secret algorithm [Re: Clipper Chip and crypto\n",
      "\n",
      "3. From: keith@cco.caltech.edu (Keith Allan Schneider)\n",
      "Subject: Re: <Political Atheists?\n",
      "Organization: \n",
      "\n",
      "4. From: cdt@sw.stratus.com (C. D. Tavares)\n",
      "Subject: Re: The 'pill' for Deer = No Hunting\n",
      "Organization:\n",
      "\n",
      "5. From: dbrooks@osf.org (David Brooks)\n",
      "Subject: Re: Q: Colormaps with dialog shells\n",
      "Organization: Open\n"
     ]
    }
   ],
   "source": [
    "\n",
    "idx = np.random.randint(0, len(data['data']))\n",
    "print('\\nQuery Document to search: \\n\\n: ', data['data'][idx][:100])\n",
    "\n",
    "doc = doc_sim(idx)\n",
    "\n",
    "print(\"\\nList of Most Similar Documents by ranking: \\n\")\n",
    "for i in range (len(doc)):\n",
    "    print(f'\\n{i+1}. {doc[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPr/jl0BwipGfOdh04SJjyQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
